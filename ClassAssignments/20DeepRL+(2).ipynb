{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20DeepRL.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1gx3XlhM3LDC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**20 Deep RL**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GeFHGQmxINLG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This exercise is an open-ended task for you to explore a method of your choice in deep RL. The submission is a code sample of your choice with instructions on how to run it where applicable (.py or .js file).\n",
        "\n",
        "You can select one of the tutorials below or find your own that you find exciting or educational. Follow the tutorial and make some modifications that you find interesting. Write a comment in your code that describes how you deviated from following the exact tutorial. You should change some aspect of the code compared to the original tutorial and describe what changed to receive full credit - this could be as simple as hyperparameter tuning."
      ]
    },
    {
      "metadata": {
        "id": "IHMKf-jS3XHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Deep Reinforcement Learning Tennis Example** \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aMywO5et3hQy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this environment, two agents control the rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receivers reward of -0.1. Thus the goal of each agent is to keep the ball in play. In this case, I have change the code with its different hyperparameter tuning in the environment to when the ball hits over the net, it receives a reward of +0.1, and when the ball hit the groun or hits the ball out of bounds, it receives reward of -0.01. This seems to be more effective as the previous reward system has a higher difference between the +1 and -0.1, prioritizing the reward. In this new hyperparameter, the lower difference between the reward hyperparameters reinforces the agent to recieve a higher reward.\n",
        "\n",
        "The goal of this program is to train my own agent to solve the environment. When training the envrionment, the train_mode is set to True.\n",
        "\n",
        "source: https://github.com/udacity/deep-reinforcement-learning/blob/master/p3_collab-compet/Soccer.ipynb\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NQZ59o6i--K7",
        "colab_type": "code",
        "outputId": "82cb111a-128c-427a-a5d8-a7ede14a5f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install keras"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s9ZN5H7J_Ivq",
        "colab_type": "code",
        "outputId": "505e92e6-8050-462d-86c4-f1f5a003d8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/keras-team/keras.git\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/22)   \u001b[K\rremote: Counting objects:   9% (2/22)   \u001b[K\rremote: Counting objects:  13% (3/22)   \u001b[K\rremote: Counting objects:  18% (4/22)   \u001b[K\rremote: Counting objects:  22% (5/22)   \u001b[K\rremote: Counting objects:  27% (6/22)   \u001b[K\rremote: Counting objects:  31% (7/22)   \u001b[K\rremote: Counting objects:  36% (8/22)   \u001b[K\rremote: Counting objects:  40% (9/22)   \u001b[K\rremote: Counting objects:  45% (10/22)   \u001b[K\rremote: Counting objects:  50% (11/22)   \u001b[K\rremote: Counting objects:  54% (12/22)   \u001b[K\rremote: Counting objects:  59% (13/22)   \u001b[K\rremote: Counting objects:  63% (14/22)   \u001b[K\rremote: Counting objects:  68% (15/22)   \u001b[K\rremote: Counting objects:  72% (16/22)   \u001b[K\rremote: Counting objects:  77% (17/22)   \u001b[K\rremote: Counting objects:  81% (18/22)   \u001b[K\rremote: Counting objects:  86% (19/22)   \u001b[K\rremote: Counting objects:  90% (20/22)   \u001b[K\rremote: Counting objects:  95% (21/22)   \u001b[K\rremote: Counting objects: 100% (22/22)   \u001b[K\rremote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/19)   \u001b[K\rremote: Compressing objects:  10% (2/19)   \u001b[K\rremote: Compressing objects:  15% (3/19)   \u001b[K\rremote: Compressing objects:  21% (4/19)   \u001b[K\rremote: Compressing objects:  26% (5/19)   \u001b[K\rremote: Compressing objects:  31% (6/19)   \u001b[K\rremote: Compressing objects:  36% (7/19)   \u001b[K\rremote: Compressing objects:  42% (8/19)   \u001b[K\rremote: Compressing objects:  47% (9/19)   \u001b[K\rremote: Compressing objects:  52% (10/19)   \u001b[K\rremote: Compressing objects:  57% (11/19)   \u001b[K\rremote: Compressing objects:  63% (12/19)   \u001b[K\rremote: Compressing objects:  68% (13/19)   \u001b[K\rremote: Compressing objects:  73% (14/19)   \u001b[K\rremote: Compressing objects:  78% (15/19)   \u001b[K\rremote: Compressing objects:  84% (16/19)   \u001b[K\rremote: Compressing objects:  89% (17/19)   \u001b[K\rremote: Compressing objects:  94% (18/19)   \u001b[K\rremote: Compressing objects: 100% (19/19)   \u001b[K\rremote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "Receiving objects:   0% (1/30142)   \rReceiving objects:   1% (302/30142)   \rReceiving objects:   2% (603/30142)   \rReceiving objects:   3% (905/30142)   \rReceiving objects:   4% (1206/30142)   \rReceiving objects:   5% (1508/30142)   \rReceiving objects:   6% (1809/30142)   \rReceiving objects:   7% (2110/30142)   \rReceiving objects:   8% (2412/30142)   \rReceiving objects:   9% (2713/30142)   \rReceiving objects:  10% (3015/30142)   \rReceiving objects:  11% (3316/30142)   \rReceiving objects:  12% (3618/30142)   \rReceiving objects:  13% (3919/30142)   \rReceiving objects:  14% (4220/30142)   \rReceiving objects:  15% (4522/30142)   \rReceiving objects:  16% (4823/30142)   \rReceiving objects:  17% (5125/30142)   \rReceiving objects:  18% (5426/30142)   \rReceiving objects:  19% (5727/30142)   \rReceiving objects:  20% (6029/30142)   \rReceiving objects:  21% (6330/30142)   \rReceiving objects:  22% (6632/30142)   \rReceiving objects:  23% (6933/30142)   \rReceiving objects:  24% (7235/30142)   \rReceiving objects:  25% (7536/30142)   \rReceiving objects:  26% (7837/30142)   \rReceiving objects:  27% (8139/30142)   \rReceiving objects:  28% (8440/30142)   \rReceiving objects:  29% (8742/30142)   \rReceiving objects:  30% (9043/30142)   \rReceiving objects:  31% (9345/30142)   \rReceiving objects:  32% (9646/30142)   \rReceiving objects:  33% (9947/30142)   \rReceiving objects:  34% (10249/30142)   \rReceiving objects:  35% (10550/30142)   \rReceiving objects:  36% (10852/30142)   \rReceiving objects:  37% (11153/30142)   \rReceiving objects:  38% (11454/30142)   \rReceiving objects:  39% (11756/30142)   \rReceiving objects:  40% (12057/30142)   \rReceiving objects:  41% (12359/30142)   \rReceiving objects:  42% (12660/30142)   \rReceiving objects:  43% (12962/30142)   \rReceiving objects:  44% (13263/30142)   \rReceiving objects:  45% (13564/30142)   \rReceiving objects:  46% (13866/30142)   \rReceiving objects:  47% (14167/30142)   \rReceiving objects:  48% (14469/30142)   \rReceiving objects:  49% (14770/30142)   \rReceiving objects:  50% (15071/30142)   \rReceiving objects:  51% (15373/30142)   \rReceiving objects:  52% (15674/30142)   \rReceiving objects:  53% (15976/30142)   \rReceiving objects:  54% (16277/30142)   \rReceiving objects:  55% (16579/30142)   \rReceiving objects:  56% (16880/30142)   \rReceiving objects:  57% (17181/30142)   \rReceiving objects:  58% (17483/30142)   \rReceiving objects:  59% (17784/30142)   \rReceiving objects:  60% (18086/30142)   \rReceiving objects:  61% (18387/30142)   \rReceiving objects:  62% (18689/30142)   \rReceiving objects:  63% (18990/30142)   \rReceiving objects:  64% (19291/30142)   \rReceiving objects:  65% (19593/30142)   \rReceiving objects:  66% (19894/30142)   \rReceiving objects:  67% (20196/30142)   \rReceiving objects:  68% (20497/30142)   \rReceiving objects:  69% (20798/30142)   \rReceiving objects:  70% (21100/30142)   \rReceiving objects:  71% (21401/30142)   \rReceiving objects:  72% (21703/30142)   \rReceiving objects:  73% (22004/30142)   \rReceiving objects:  74% (22306/30142)   \rReceiving objects:  75% (22607/30142)   \rReceiving objects:  76% (22908/30142)   \rReceiving objects:  77% (23210/30142)   \rReceiving objects:  78% (23511/30142)   \rReceiving objects:  79% (23813/30142)   \rReceiving objects:  80% (24114/30142)   \rReceiving objects:  81% (24416/30142)   \rReceiving objects:  82% (24717/30142)   \rReceiving objects:  83% (25018/30142)   \rReceiving objects:  84% (25320/30142)   \rReceiving objects:  85% (25621/30142)   \rReceiving objects:  86% (25923/30142)   \rReceiving objects:  87% (26224/30142)   \rReceiving objects:  88% (26525/30142)   \rReceiving objects:  89% (26827/30142)   \rReceiving objects:  90% (27128/30142)   \rReceiving objects:  91% (27430/30142)   \rReceiving objects:  92% (27731/30142)   \rReceiving objects:  93% (28033/30142)   \rReceiving objects:  94% (28334/30142)   \rReceiving objects:  95% (28635/30142)   \rReceiving objects:  96% (28937/30142)   \rReceiving objects:  97% (29238/30142)   \rReceiving objects:  98% (29540/30142)   \rReceiving objects:  99% (29841/30142)   \rremote: Total 30142 (delta 6), reused 7 (delta 3), pack-reused 30120\u001b[K\n",
            "Receiving objects: 100% (30142/30142), 11.74 MiB | 23.57 MiB/s, done.\n",
            "Resolving deltas: 100% (21934/21934), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EzlPAfNU_OQk",
        "colab_type": "code",
        "outputId": "b16c9d50-3fb3-403d-e78e-a399d4af0495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "! python3 -m pip install -U pygame --user\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pygame in /root/.local/lib/python3.6/site-packages (1.9.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UEV_te4p_Yyy",
        "colab_type": "code",
        "outputId": "f0b390a3-8415-4884-e16d-d78491a1d060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "! python3 -m pygame.examples.aliens\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.4\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "Warning, no sound\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/root/.local/lib/python3.6/site-packages/pygame/examples/aliens.py\", line 321, in <module>\n",
            "    if __name__ == '__main__': main()\n",
            "  File \"/root/.local/lib/python3.6/site-packages/pygame/examples/aliens.py\", line 188, in main\n",
            "    bestdepth = pygame.display.mode_ok(SCREENRECT.size, winstyle, 32)\n",
            "pygame.error: video system not initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cy7FDGOCBKHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "621c67d0-bd7e-4287-85c9-853e59917c7a"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Unity-Technologies/ml-agents.git\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-agents'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 11714 (delta 2), reused 1 (delta 0), pack-reused 11701\u001b[K\n",
            "Receiving objects: 100% (11714/11714), 148.34 MiB | 30.00 MiB/s, done.\n",
            "Resolving deltas: 100% (7720/7720), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K68bYW-H-oi4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# installed unity in computer\n",
        "from unityagents import UnityEnvironment\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZVjO7GzA9bD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = UnityEnvironment(file_name=\"Tennis.exe\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhuV4IHXCfgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get the default brain\n",
        "brain_name = env.brain_names[0]\n",
        "brain = env.brains[brain_name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAL1OvWMHBuk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "    \n",
        "    for i in range(5):\n",
        "        current_state = env.reset()\n",
        "\n",
        "        tot_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        while not done:\n",
        "            \n",
        "            method = params[\"epsilon\"]\n",
        "            epsilon = params[0.3]\n",
        "            alpha = params[0.1]\n",
        "            gamma = params[0.5]\n",
        "            \n",
        "            action = select_action(q_table[current_state], method, epsilon)   \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            q_table[current_state, action] = calculate_new_q_val(q_table, current_state, action, reward, next_state, alpha, gamma)\n",
        "\n",
        "            \n",
        "            tot_reward += reward\n",
        "            step +=1\n",
        "            if render:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Simulation: {i + 1}\")\n",
        "                env.render()\n",
        "                print(f\"Step: {step}\")\n",
        "                print(f\"Current State: {current_state}\")\n",
        "                print(f\"Action: {action}\")\n",
        "                print(f\"Reward: {reward}\")\n",
        "                print(f\"Total rewards: {tot_reward}\")\n",
        "                sleep(.2)\n",
        "            if step == 50:\n",
        "                print(\"Agent got stuck. Quitting...\")\n",
        "                sleep(.5)\n",
        "                break\n",
        "        \n",
        "        rewards.append(tot_reward)\n",
        "    \n",
        "    return np.array(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMYRCWIgCiYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9377d105-021a-4468-ef03-1d68c8ffb906"
      },
      "cell_type": "code",
      "source": [
        "#for i in range(1, 6):                                      # play game for 5 episodes\n",
        "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
        "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
        "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
        "    while True:\n",
        "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
        "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
        "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
        "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
        "        rewards = env_info.rewards                         # get reward (for each agent)\n",
        "        dones = env_info.local_done                        # see if episode finished\n",
        "        scores += env_info.rewards                         # update the score (for each agent)\n",
        "        states = next_states                               # roll over states to next time step\n",
        "        if np.any(dones):                                  # exit loop if episode finished\n",
        "            break\n",
        "    print('Scpre (max over agents) from episode {}: {}'.format(i, np.max(scores)))\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scpre (max over agents) from episode 5: 3 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OU_gu4jwCmw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E3IVUEP2FRiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}